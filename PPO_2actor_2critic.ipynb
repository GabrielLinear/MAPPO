{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import collections\n",
    "from multiprocessing import Process\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/gabyc/Desktop/Reinforcment_TP/deep-reinforcement-learning/p3_collab-compet/Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    # Actor network \n",
    "    def __init__(self,input_size,nb_action):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nb_action = nb_action\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,nb_action)\n",
    "        self.sigma = nn.Linear(256,nb_action)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.fc3(x)) # Tanh because action_values between -1 and 1.\n",
    "\n",
    "        # Making stochastic policy\n",
    "        #log_sigma = -torch.relu(self.sigma(x))\n",
    "        #sigma = torch.exp(log_sigma)\n",
    "        sigma = torch.ones(self.nb_action,requires_grad=False).to(self.device)/2 # Variance of 0.5 for each action\n",
    "        m = torch.distributions.normal.Normal(mu,sigma,False)# False, whereas constraint on mu = 0\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    # Critic network \n",
    "    def __init__(self,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,1) # 1 output -> Value estimate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  F.leaky_relu(self.fc3(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_prob(policy,states,actions,device):\n",
    "    # The Gradient FLOW on action\n",
    "    # The Gradient fon't FLOW on state \n",
    "    # No Clipping.\n",
    "    Tab = []\n",
    "    Action_sample_tab = []\n",
    "    m = policy(states[0])\n",
    "    \n",
    "    proba = m.log_prob(actions[0])\n",
    "\n",
    "    # STORE\n",
    "    Tab.append(proba)\n",
    "    Action_sample_tab.append(actions[0])\n",
    "    \n",
    "    # Loop over the state and action (a,s)\n",
    "    for state_iter,action_iter in zip(states[1:],actions[1:]):\n",
    "        m = policy(state_iter)\n",
    "        proba = m.log_prob(action_iter) # Prob on the previous action but new policy\n",
    "   \n",
    "        # STORE\n",
    "        Tab.append(proba)\n",
    "        Action_sample_tab.append(action_iter)\n",
    "\n",
    "    return torch.stack(Tab),torch.stack(Action_sample_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(Delta_t,critic,optimizer,device,policy, old_probs,actions, states,state_full, rewards,batch_size,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    \n",
    "    # Convert REWARD TO REWARD FUTURE\n",
    "    rewards = np.asarray(rewards)\n",
    "\n",
    "    reward_futur = np.zeros(rewards.shape[0])\n",
    "    longueur = rewards.shape[0] - 1\n",
    "    reward_futur[longueur] = rewards[longueur]\n",
    "    new_discount = 0\n",
    "    for i in range(1,rewards.shape[0]):\n",
    "        new_discount = discount**(longueur-i) \n",
    "        reward_futur[longueur-i] = reward_futur[longueur-(i-1)] + rewards[longueur-i]*new_discount\n",
    "    \n",
    "    \n",
    "    #Normalize At\n",
    "    Delta_t = Delta_t.detach()\n",
    "    Delta_t = Delta_t[:,None].repeat(1, old_probs.shape[1])\n",
    "    Delta_t = (Delta_t- Delta_t.mean())/(Delta_t.std() + 1e-10)\n",
    "    \n",
    "    new_prob,action_sample = New_prob(policy, states,actions,device)\n",
    "    \n",
    "    # Compute each \n",
    "    Fraction = torch.exp(new_prob-(old_probs+1e-10))\n",
    "    Cote1 = Delta_t*Fraction \n",
    "    Cote2 = Delta_t*torch.clamp(Fraction, 1-epsilon, 1+epsilon) \n",
    "    Cote1 = Cote1[:, :,None]\n",
    "    Cote2 = Cote2[:, :,None]\n",
    "    comp = torch.cat((Cote1, Cote2),2)\n",
    "    Gradient = torch.min(comp,2)[0].to(device) # Surrogate function\n",
    "\n",
    "\n",
    "    entropy = -(torch.exp(new_prob)*old_probs+1.e-10)+ \\\n",
    "        (1.0-torch.exp(new_prob))*(1.0-old_probs+1.e-10) # Entropy to enhance exploration\n",
    "\n",
    "    writer.add_scalar('Entropy',torch.mean(beta*(entropy)),iteration_all)\n",
    "    writer.add_scalar('Gradient',torch.mean(Gradient),iteration_all)\n",
    "    \n",
    "    MSE = TD_Training(critic,optimizer,state_full,reward_futur,discount,device) # Critic network training\n",
    "    writer.add_scalar('Loss/Critic',MSE,iteration_all)\n",
    "\n",
    "    return -torch.mean(beta*(entropy) + Gradient)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_Training(Critic,optimizereer,states,reward,discount,device):\n",
    "    # Function for training the critic\n",
    "    states = states.detach()\n",
    "    reward = torch.from_numpy(reward).detach()\n",
    "    value_loss = []\n",
    "    for st in states:\n",
    "        Valuet = Critic(st.flatten())\n",
    "        value_loss.append(Valuet)\n",
    "    \n",
    "    Loss = 0.5*(discount*reward.to(device)[:,None] - torch.stack(value_loss)).pow(2).mean() # Simple MSE Loss\n",
    "    optimizereer.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizereer.step()\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories_double(env,env_info,policy,policy2,device,tmax):\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    state = env_info.vector_observations # get the current state (for each agent)\n",
    "    states_tab = []\n",
    "    states_tab1 , action_tab1, reward_tab1, prob_tab1,done1 = [],[],[], [], []\n",
    "    states_tab2 , action_tab2, reward_tab2, prob_tab2,done2 = [],[],[], [], []\n",
    "    t = 0\n",
    "    reward_episode = []\n",
    "    while t < tmax:\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        #state = torch.from_numpy(state).to(device)\n",
    "        score = np.zeros(2)\n",
    "        while True:\n",
    "            policy.eval()\n",
    "            policy2.eval()\n",
    "            with torch.no_grad(): # Everything with torch no grad.\n",
    "                m = policy(torch.from_numpy(state[0]).to(device))\n",
    "                m2 = policy2(torch.from_numpy(state[1]).to(device))\n",
    "                \n",
    "\n",
    "                # Sample maybe on gradient as to check that\n",
    "                sample = m.sample()\n",
    "                sample2 = m2.sample()\n",
    "                action_tab1.append(sample)\n",
    "                action_tab2.append(sample2)# No clip and store\n",
    "                states_tab.append(torch.from_numpy(state))\n",
    "                #states_tab.append(torch.cat((torch.from_numpy(state),sample[:,None],sample2[:,None],),axis=1))\n",
    "\n",
    "                # Proba not on clip and detach from Gradient.\n",
    "                proba = m.log_prob(sample)\n",
    "                proba2 = m2.log_prob(sample2)\n",
    "                #proba = torch.exp(proba) #Proba on CUDA no detach\n",
    "\n",
    "                # Interact with the environment \n",
    "                sample = torch.clip(sample.detach().cpu(), -1, 1) # CLIP BEFORE TAKING THE PROBA OR AFTER?\n",
    "                sample = sample.numpy()\n",
    "\n",
    "                sample2 = torch.clip(sample2.detach().cpu(), -1, 1) # CLIP BEFORE TAKING THE PROBA OR AFTER?\n",
    "                sample2 = sample2.numpy()\n",
    "\n",
    "\n",
    "                action_env = np.concatenate([np.expand_dims(sample,axis=0),np.expand_dims(sample2,axis=0)],axis=0)\n",
    "\n",
    "                # Step the environment\n",
    "                env_info = env.step(action_env)[brain_name]           # send all actions to the environment\n",
    "                next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "                rewards = env_info.rewards                         # get reward (for each agent)\n",
    "                dones = env_info.local_done                        # see if episode finished           \n",
    " \n",
    "                score += rewards\n",
    "\n",
    "                # Store values\n",
    "                prob_tab1.append(proba)\n",
    "                prob_tab2.append(proba2)\n",
    "\n",
    "                reward_tab1.append(np.asarray(rewards)[0])\n",
    "                reward_tab2.append(np.asarray(rewards)[1])\n",
    "                states_tab1.append(torch.from_numpy(state[0]).to(device))\n",
    "                states_tab2.append(torch.from_numpy(state[1]).to(device))\n",
    "\n",
    "                # BREAK IF END OF THE EPISODE\n",
    "                if np.any(dones):                                  # exit loop if episode finished\n",
    "                    reward_episode.append(score)\n",
    "                    break\n",
    "                if t >= tmax:\n",
    "                    reward_episode.append(score)\n",
    "                    break\n",
    "                state = next_states\n",
    "                t +=1\n",
    "            \n",
    "    return states_tab, states_tab1 , action_tab1, reward_tab1, prob_tab1,done1, states_tab2 , action_tab2, reward_tab2, prob_tab2,done2,reward_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(Critic,states,reward,discount,device):\n",
    "    # Calculate TD error during the evaluation step\n",
    "    Delta_t = []\n",
    "    Tab = []\n",
    "    Critic.eval()\n",
    "    with torch.no_grad(): \n",
    "        Valuet = Critic(states[0].flatten())\n",
    "    \n",
    "        for rw,st in zip(reward[0:],states[1:]):\n",
    "            Valuetplus1 = Critic(st.flatten())\n",
    "            Tab.append(Valuetplus1)\n",
    "\n",
    "            TD_error = torch.from_numpy(np.asarray(rw)).to(device) + discount*Valuetplus1[0] - Valuet[0] #TD ERROR\n",
    "            Delta_t.append(TD_error)\n",
    "            \n",
    "            Valuet = Valuetplus1\n",
    "    writer.add_histogram('Values',torch.stack(Tab),e)\n",
    "    return torch.stack(Delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAE_evaluation(Delta_t,discount,lambd):\n",
    "    # GAE Function adapted from https://github.com/numblr/drlnd-cocontrol\n",
    "    flipped = torch.flip(Delta_t, dims=(0,))\n",
    "    result = torch.zeros_like(flipped)\n",
    "    result[0] = flipped[0]\n",
    "    for i in range(1, flipped.size()[0]):\n",
    "        result[i] = discount * lambd * result[i-1] + flipped[i]\n",
    "\n",
    "    return torch.flip(result, dims=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "from collections import deque\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]  \n",
    "states = env_info.vector_observations # get the current state (for each agent\n",
    "num_agents = len(states)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "nb_states = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy = Policy(nb_states,action_size).to(device) # Policy network\n",
    "policy.load_state_dict(torch.load(\"PPO_actor_stable.pth\"))\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "critic = Critic(nb_states*2).to(device) # Critic network\n",
    "critic.load_state_dict(torch.load(\"PPO_critic_stable.pth\"))\n",
    "optimizer_c = optim.Adam(critic.parameters(), lr=1e-4)\n",
    "\n",
    "policy2 = Policy(nb_states,action_size).to(device) # Policy network\n",
    "policy2.load_state_dict(torch.load(\"PPO_actor2_stable.pth\"))\n",
    "optimizer2 = optim.Adam(policy2.parameters(), lr=1e-4)\n",
    "critic2 = Critic(nb_states*2).to(device) # Critic network\n",
    "critic2.load_state_dict(torch.load(\"PPO_critic2_stable.pth\"))\n",
    "optimizer_c2 = optim.Adam(critic2.parameters(), lr=1e-4)\n",
    "# 1e-4 work well\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Episode: 20, score: 5.465000\n",
      "5.465000086463988\n",
      "################################\n",
      "Episode: 40, score: 5.465000\n",
      "5.46500008739531\n",
      "################################\n",
      "Episode: 60, score: 5.695000\n",
      "5.695000088773668\n",
      "################################\n",
      "Episode: 80, score: 5.295000\n",
      "5.2950000846758485\n",
      "################################\n",
      "Episode: 100, score: 5.685000\n",
      "5.685000088997185\n",
      "################################\n",
      "Episode: 120, score: 6.020000\n",
      "6.020000092685223\n",
      "################################\n",
      "Episode: 140, score: 5.630000\n",
      "5.630000088363886\n",
      "################################\n",
      "Episode: 160, score: 5.510000\n",
      "5.510000088252127\n",
      "################################\n",
      "Episode: 180, score: 5.980000\n",
      "5.980000091716647\n",
      "################################\n",
      "Episode: 200, score: 5.360000\n",
      "5.36000008508563\n",
      "################################\n",
      "Episode: 220, score: 5.980000\n",
      "5.98000009264797\n"
     ]
    }
   ],
   "source": [
    "###################################################### MAIN_CODE #################################################\n",
    "# training loop max iterations\n",
    "episode = 5000\n",
    "\n",
    "tmax = 2500 # 2500\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "SGD_epoch = 2\n",
    "batch_size = 512 #128\n",
    "lambd = 0.95\n",
    "\n",
    "# keep track of progress\n",
    "aleatoire = True\n",
    "mean_rewards = []\n",
    "writer.add_text(\"CONFIG\",\"Critic_nbstate*2, 1e-4, after loading\" + str(aleatoire) + \"tmax :\" + str(tmax) + \"batch_size :\" + str(batch_size) + \"discount_rate :\" + str(discount_rate) + \"epsilon\" + str(epsilon)+ \"beta\" + str(beta) + \"SGD_epoch :\" + str(SGD_epoch) + \"lambd :\" + str(lambd) + \"lr : 2e-4 x2\")\n",
    "iteration_all = 0\n",
    "\n",
    "itbis = 0\n",
    "scores_deque = deque(maxlen=100)\n",
    "for e in range(episode):\n",
    "   # if beta <= 0.01:\n",
    "   #     beta = 0.01\n",
    "    #else :\n",
    "    #    beta = beta*.99\n",
    "    writer.add_scalar('beta_decrease',beta,e)\n",
    "    # EVALUATION STEP\n",
    "    # collect trajectories\n",
    "    states_tab, states1 , action1, reward1, prob1, done1, states2 , action2, reward2, prob2, done2,reward_episode = collect_trajectories_double(env,env_info, policy,policy2, device,tmax)\n",
    "    total_rewards = np.mean(np.sum(reward_episode,axis=0))\n",
    "\n",
    "    for r in reward_episode:\n",
    "        itbis+=1\n",
    "        writer.add_scalar('Score_agent1',r[0],itbis)\n",
    "        writer.add_scalar('Score_agent2',r[1],itbis)\n",
    "        scores_deque.append(np.max(r))\n",
    "        writer.add_scalar('Score_espisode_mean',np.mean(scores_deque),itbis)\n",
    "    \n",
    "    # Compute advantages estimate for first agent\n",
    "    Delta_t1 = TD_evaluation(critic,states_tab,reward1,discount_rate,device)\n",
    "    writer.add_scalar('DeltaT1',torch.mean(Delta_t1),iteration_all)\n",
    "    Delta_t1 = GAE_evaluation(Delta_t1,discount_rate,lambd)\n",
    "    writer.add_scalar('Advantage1',torch.mean(Delta_t1),iteration_all)\n",
    "    \n",
    "    # Compute advantages estimate for the second agent\n",
    "    Delta_t2 = TD_evaluation(critic2,states_tab,reward2,discount_rate,device)\n",
    "    writer.add_scalar('DeltaT2',torch.mean(Delta_t2),iteration_all)\n",
    "    Delta_t2 = GAE_evaluation(Delta_t2,discount_rate,lambd)\n",
    "    writer.add_scalar('Advantage2',torch.mean(Delta_t2),iteration_all)\n",
    "    \n",
    "    # To keep trajectories and deltaT accordingly for first agent\n",
    "    states1 = torch.stack(states1)[:-1]\n",
    "    action1 = torch.stack(action1)[:-1]\n",
    "    prob1 = torch.stack(prob1)[:-1]\n",
    "    reward1 = np.asarray(reward1)[:-1]\n",
    "    \n",
    "    # To keep trajectories and deltaT accordingly for second agent\n",
    "    states2 = torch.stack(states2)[:-1]\n",
    "    action2 = torch.stack(action2)[:-1]\n",
    "    prob2 = torch.stack(prob2)[:-1]\n",
    "    reward2 = np.asarray(reward2)[:-1]\n",
    "    \n",
    "    states_tab = torch.stack(states_tab)[:-1]\n",
    "    \n",
    "    # TRAINING STEP\n",
    "    indices = torch.split(torch.from_numpy(np.arange(0,states1.shape[0],1)),batch_size,0) # Make chunk of the trajectory\n",
    "    for epoch in range(SGD_epoch):\n",
    "        # TRAINING OVER THE BATCH SIZE\n",
    "        for chunks in indices:\n",
    "            iteration_all += 1\n",
    "            chunk = chunks.long()\n",
    "            chunk_numpy = chunk.numpy().astype('int')\n",
    "            \n",
    "            # Pick Chunk of the trajectories\n",
    "            states_chunk1 = states1[chunk]\n",
    "            actions_chunk1 = action1[chunk]\n",
    "            prob_chunk1 = prob1[chunk]\n",
    "            rewards_chunk1 = reward1[chunk_numpy]\n",
    "            Delta_t_chunk1 = Delta_t1[chunk]\n",
    "            rewards_chunk1 = rewards_chunk1.tolist()\n",
    "            \n",
    "            states_chunk2 = states2[chunk]\n",
    "            actions_chunk2 = action2[chunk]\n",
    "            prob_chunk2 = prob2[chunk]\n",
    "            rewards_chunk2 = reward2[chunk_numpy]\n",
    "            Delta_t_chunk2 = Delta_t2[chunk]\n",
    "            rewards_chunk2 = rewards_chunk2.tolist()\n",
    "            \n",
    "            state_full = states_tab[chunk]\n",
    "            \n",
    "            # Learning of first agent via surrogate trainning\n",
    "            L1 = clipped_surrogate(Delta_t_chunk1,critic,optimizer_c,device,policy,prob_chunk1,actions_chunk1, states_chunk1,state_full, rewards_chunk1,batch_size, epsilon=epsilon, beta=beta)\n",
    "            optimizer.zero_grad()\n",
    "            L1.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss/Policy1',L1,iteration_all)\n",
    "            \n",
    "            # Learning of second agent via surrogate trainning\n",
    "            L2 = clipped_surrogate(Delta_t_chunk2,critic2,optimizer_c2,device,policy2,prob_chunk2,actions_chunk2, states_chunk2,state_full, rewards_chunk2,batch_size, epsilon=epsilon, beta=beta)\n",
    "            optimizer2.zero_grad()\n",
    "            L2.backward()\n",
    "            optimizer2.step()\n",
    "            writer.add_scalar('Loss/Policy2',L2,iteration_all)\n",
    "            del L1\n",
    "            del L2\n",
    "    writer.add_scalar('Score',total_rewards,e)\n",
    "            \n",
    "    mean_rewards.append(total_rewards)\n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"################################\")\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,total_rewards))\n",
    "        print(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(torch.stack(states1)[5]).sample() # DIRECT DES NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy2(torch.stack(states2)[1]).sample() # DIRECT DES NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy2.state_dict(), 'PPO_actor2_stable_finish.pth')\n",
    "torch.save(critic2.state_dict(), 'PPO_critic2_stable_finish.pth')\n",
    "\n",
    "torch.save(policy.state_dict(), 'PPO_actor_stable_finish.pth')\n",
    "torch.save(critic.state_dict(), 'PPO_critic_stable_finish.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(self, value):\n",
    "    if self._validate_args:\n",
    "        self._validate_sample(value)\n",
    "    # compute the variance\n",
    "    var = (self.scale ** 2) #VARIANCE POSITIVE OK SIGMA TOUJOURS POSITIVES NORMALEMENT.\n",
    "    log_scale = math.log(self.scale) if isinstance(self.scale, Real) else self.scale.log()\n",
    "    return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(b + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Navigation3",
   "language": "python",
   "name": "navigation3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
