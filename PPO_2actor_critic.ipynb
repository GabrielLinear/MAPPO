{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import collections\n",
    "from multiprocessing import Process\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/gabyc/Desktop/Reinforcment_TP/deep-reinforcement-learning/p3_collab-compet/Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    # Actor network \n",
    "    def __init__(self,input_size,nb_action):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nb_action = nb_action\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc1 = nn.Linear(input_size,200)\n",
    "        self.fc2 = nn.Linear(200,75)\n",
    "        self.fc2bis = nn.Linear(200,75)\n",
    "        self.fc3 = nn.Linear(75,nb_action)\n",
    "        self.fc3bis = nn.Linear(75,nb_action)\n",
    "        \n",
    "        for name, param in self.fc1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            \n",
    "        for name, param in self.fc2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        \n",
    "        for name, param in self.fc2bis.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "        for name, param in self.fc3bis.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "        for name, param in self.fc3.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x2 = x.clone()\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.fc3(x)) # Tanh because action_values between -1 and 1.\n",
    "        #sigma = F.sigmoid(self.fc2bis(x2))\n",
    "        #sigma = torch.clamp(F.sigmoid(self.fc3bis(sigma)),min=0.1)\n",
    "        sigma = torch.ones(self.nb_action,requires_grad=False).to(self.device)/4\n",
    "        m = torch.distributions.normal.Normal(mu,sigma,False)# False, whereas constraint on mu = 0\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    # Critic network \n",
    "    def __init__(self,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,150)\n",
    "        self.fc2 = nn.Linear(150,50)\n",
    "        self.fc3 = nn.Linear(50,1) # 1 output -> Value estimate\n",
    "        \n",
    "        for name, param in self.fc1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            \n",
    "        for name, param in self.fc2.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "        for name, param in self.fc3.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  F.leaky_relu(self.fc3(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_prob(policy,states,actions,device):\n",
    "    # The Gradient FLOW on action\n",
    "    # The Gradient fon't FLOW on state \n",
    "    # No Clipping.\n",
    "    Tab = []\n",
    "    Action_sample_tab = []\n",
    "    m = policy(states[0])\n",
    "    \n",
    "    proba = m.log_prob(actions[0] + 1e-10)\n",
    "\n",
    "    # STORE\n",
    "    Tab.append(proba)\n",
    "    Action_sample_tab.append(actions[0])\n",
    "    \n",
    "    # Loop over the state and action (a,s)\n",
    "    for state_iter,action_iter in zip(states[1:],actions[1:]):\n",
    "        m = policy(state_iter)\n",
    "        proba = m.log_prob(action_iter + 1e-10) # Prob on the previous action but new policy\n",
    "   \n",
    "        # STORE\n",
    "        Tab.append(proba)\n",
    "        Action_sample_tab.append(action_iter)\n",
    "\n",
    "    return torch.stack(Tab),torch.stack(Action_sample_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(Delta_t,critic,device,policy, old_probs,actions, states, rewards,batch_size,nb_agent,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    \n",
    "    rewards = np.asarray(rewards).mean(axis=1)\n",
    "    old_probs_extract = old_probs[:,nb_agent,:]\n",
    "    states_extract = states[:,nb_agent,:]\n",
    "    actions_extract = actions[:,nb_agent,:]\n",
    "\n",
    "    \n",
    "    # Convert REWARD TO REWARD FUTURE\n",
    "    reward_futur = np.zeros(rewards.shape[0])\n",
    "    longueur = rewards.shape[0] - 1\n",
    "    reward_futur[longueur] = rewards[longueur]\n",
    "    new_discount = 0\n",
    "    for i in range(1,rewards.shape[0]):\n",
    "        new_discount = discount**(longueur-i) \n",
    "        reward_futur[longueur-i] = reward_futur[longueur-(i-1)] + rewards[longueur-i]*new_discount\n",
    "        \n",
    "    #Normalize At\n",
    "    Delta_t = Delta_t.detach()\n",
    "    Delta_t = Delta_t[:,None].repeat(1, old_probs.shape[2])\n",
    "    Delta_t = (Delta_t- Delta_t.mean())/Delta_t.std()\n",
    "    \n",
    "    new_prob,action_sample = New_prob(policy, states_extract,actions_extract,device)\n",
    "    \n",
    "    # Compute each \n",
    "    Fraction = torch.exp(new_prob-(old_probs_extract+1e-10))\n",
    "    Cote1 = Delta_t*Fraction \n",
    "    Cote2 = Delta_t*torch.clamp(Fraction, 1-epsilon, 1+epsilon) \n",
    "    Cote1 = Cote1[:, :,None]\n",
    "    Cote2 = Cote2[:, :,None]\n",
    "    comp = torch.cat((Cote1, Cote2),2)\n",
    "    Gradient = torch.min(comp,2)[0].to(device) # Surrogate function\n",
    "\n",
    "\n",
    "    entropy = -(torch.exp(new_prob)*old_probs_extract+1.e-10)+ \\\n",
    "        (1.0-torch.exp(new_prob))*(1.0-old_probs_extract+1.e-10) # Entropy to enhance exploration\n",
    "\n",
    "    writer.add_scalar('Entropy',torch.mean(beta*(entropy)),iteration_all)\n",
    "    writer.add_scalar('Gradient',torch.mean(Gradient),iteration_all)\n",
    "    \n",
    "    # ATTENTION ENTRAINE DEUX FOIS.\n",
    "    # REWARD TRAINING ON BOTH REWARD.\n",
    "    MSE = TD_Training(critic,states,rewards,actions,discount,device) # Critic network training\n",
    "    writer.add_scalar('Loss/Critic',MSE,iteration_all)\n",
    "\n",
    "    return -torch.mean(beta*(entropy) + Gradient)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_Training(Critic,states,reward,actions,discount,device):\n",
    "    states = states.detach()\n",
    "    reward = torch.from_numpy(reward).detach()\n",
    "    value_loss = []\n",
    "    for st,ac in zip(states,actions):\n",
    "        stack = torch.cat((st.flatten(),ac.flatten()),axis=0)\n",
    "        Valuet = Critic(stack)\n",
    "        value_loss.append(Valuet)\n",
    "    \n",
    "    #print(\"TD_Train\")\n",
    "    #print(torch.stack(value_loss).shape)\n",
    "    #print(reward.to(device).mean(axis=1)[:,None].shape)\n",
    "    # Mean or Not Mean\n",
    "    Loss = 0.5*(discount*reward.to(device)[:,None] - torch.stack(value_loss)).pow(2).mean() # Simple MSE Loss\n",
    "    optimizer_c.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer_c.step()\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env,env_info,policy1,policy2,device,tmax):\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "     # get the current state (for each agent)\n",
    "    states_tab , action_tab, reward_tab, prob_tab = [],[],[], []\n",
    "    reward_episode = []\n",
    "    t = 0\n",
    "    while t < tmax:\n",
    "        reward_episode_temp = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        state = env_info.vector_observations # A VERIFIER CALAGE ACTIONS /STATES\n",
    "        while True:\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            policy1.eval()\n",
    "            policy2.eval()\n",
    "            with torch.no_grad(): # Everything with torch no grad.\n",
    "\n",
    "                ################ FIRST AND SECOND AGENT ################\n",
    "                m = policy1(state[0])\n",
    "                m2 = policy2(state[1])\n",
    "\n",
    "                # Sample 1 and 2\n",
    "                sample = m.sample()\n",
    "                sample2 = m2.sample()\n",
    "                action_tab.append(torch.stack([sample,sample2])) # No clip and store\n",
    "\n",
    "                # Proba not on clip and detach from Gradient.\n",
    "                proba = m.log_prob(sample)\n",
    "                proba2 = m2.log_prob(sample2)\n",
    "\n",
    "                # Interact with the environment \n",
    "                sample = torch.clip(sample.detach().cpu(), -1, 1)\n",
    "                sample2 = torch.clip(sample2.detach().cpu(), -1, 1)\n",
    "                sample = np.expand_dims(sample.numpy(),axis=0)\n",
    "                sample2 = np.expand_dims(sample2.numpy(),axis=0)\n",
    "                sample = np.concatenate([sample,sample2],axis=0)\n",
    "\n",
    "                # Step the environment\n",
    "                env_info = env.step(sample)[brain_name]           # send all actions to the environment\n",
    "                next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "                rewards = env_info.rewards                         # get reward (for each agent)\n",
    "                dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "                # Store values\n",
    "                prob_tab.append(torch.stack([proba,proba2]))\n",
    "                reward_tab.append(np.asarray(rewards))\n",
    "                states_tab.append(state)\n",
    "                reward_episode_temp.append(np.asarray(rewards))\n",
    "\n",
    "                # BREAK IF END OF THE EPISODE\n",
    "                if np.any(dones):                                  # exit loop if episode finished\n",
    "                    reward_episode.append(np.sum(reward_episode_temp,axis=0))\n",
    "                    # Write np max after to see the reward of the two agent\n",
    "                    break\n",
    "                if t >= tmax:\n",
    "                    break\n",
    "                state = next_states\n",
    "                t +=1\n",
    "    #writer.add_histogram('MU/Sample_mu_action0',torch.mean(torch.stack(action_tab)[:,:,0],axis=1),iteration_all)\n",
    "    #writer.add_histogram('MU/Sample_mu_action1',torch.mean(torch.stack(action_tab)[:,:,1],axis=1),iteration_all)\n",
    "    #writer.add_histogram('MU/Sample_mu_action2',torch.mean(torch.stack(action_tab)[:,:,2],axis=1),iteration_all)\n",
    "    #writer.add_histogram('MU/Sample_mu_action3',torch.mean(torch.stack(action_tab)[:,:,3],axis=1),iteration_all)\n",
    "    return states_tab, action_tab, reward_tab,prob_tab,reward_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(Critic,states,actions,reward,discount,device):\n",
    "    # Calculate TD error during the evaluation step\n",
    "    Delta_t = []\n",
    "    Tab = []\n",
    "    Critic.eval()\n",
    "    with torch.no_grad():\n",
    "        Valuet = Critic(torch.cat((states[0].flatten(),actions[0].flatten()),axis=0))\n",
    "    \n",
    "        for rw,st,ac in zip(reward[0:],states[1:],actions[1:]):\n",
    "            stack = torch.cat((st.flatten(),ac.flatten()),axis=0)\n",
    "            Valuetplus1 = Critic(stack)\n",
    "            Tab.append(Valuetplus1)\n",
    "            \n",
    "            # IT HAS BEEN DEALT WITH THE DIMENSION HERE\n",
    "            TD_error = torch.from_numpy(rw).to(device).mean() + discount*Valuetplus1[0] - Valuet[0] #TD ERROR\n",
    "            # CHANGE THING HERE DOESN'T CORRESPOND WELL. Not the same shape for now\n",
    "            Delta_t.append(TD_error)\n",
    "            \n",
    "            Valuet = Valuetplus1\n",
    "    print(torch.stack(Tab))\n",
    "    #writer.add_histogram('Values',torch.mean(torch.stack(Tab),axis=1),e)\n",
    "    writer.add_histogram('Values',torch.stack(Tab),e)\n",
    "    # Torch mean usefull only if two reward\n",
    "    return torch.stack(Delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAE_evaluation(Delta_t,discount,lambd):\n",
    "    # GAE Function adapted from https://github.com/numblr/drlnd-cocontrol\n",
    "    # Something strange about the dimensions here\n",
    "    #print(Delta_t.shape)\n",
    "    flipped = torch.flip(Delta_t, dims=(0,))\n",
    "    result = torch.zeros_like(flipped)\n",
    "    result[0] = flipped[0]  #Changed here 1D\n",
    "    for i in range(1, flipped.size()[0]):\n",
    "        result[i] = discount * lambd * result[i-1] + flipped[i] # Changed here 1D\n",
    "\n",
    "    return torch.flip(result, dims=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "from collections import deque\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]  \n",
    "states = env_info.vector_observations # get the current state (for each agent\n",
    "num_agents = len(states)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "nb_states = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### First Agent\n",
    "policy1 = Policy(nb_states,action_size).to(device) # Policy network\n",
    "optimizer1 = optim.Adam(policy1.parameters(), lr=4e-4)\n",
    "\n",
    "### Second Agent\n",
    "policy2 = Policy(nb_states,action_size).to(device) # Policy network\n",
    "optimizer2 = optim.Adam(policy2.parameters(), lr=4e-4)\n",
    "\n",
    "# Central Critic\n",
    "critic = Critic(nb_states*2+action_size*2).to(device) # Critic network\n",
    "optimizer_c = optim.Adam(critic.parameters(), lr=4e-4)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0058],\n",
      "        [ 0.4858],\n",
      "        [ 1.6755],\n",
      "        ...,\n",
      "        [ 3.6385],\n",
      "        [ 2.1424],\n",
      "        [ 1.9187]])\n",
      "tensor([[-0.0275],\n",
      "        [-0.0297],\n",
      "        [-0.0508],\n",
      "        ...,\n",
      "        [-0.0470],\n",
      "        [-0.0439],\n",
      "        [-0.0383]])\n",
      "tensor([[-0.0244],\n",
      "        [-0.0222],\n",
      "        [-0.0424],\n",
      "        ...,\n",
      "        [-0.0376],\n",
      "        [-0.0315],\n",
      "        [-0.0367]])\n",
      "tensor([[-0.0196],\n",
      "        [-0.0277],\n",
      "        [-0.0203],\n",
      "        ...,\n",
      "        [-0.0136],\n",
      "        [-0.0488],\n",
      "        [-0.0065]])\n",
      "tensor([[-0.0196],\n",
      "        [-0.0230],\n",
      "        [-0.0296],\n",
      "        ...,\n",
      "        [-0.0159],\n",
      "        [-0.0154],\n",
      "        [-0.0184]])\n",
      "tensor([[-0.0006],\n",
      "        [-0.0257],\n",
      "        [-0.0219],\n",
      "        ...,\n",
      "        [-0.0129],\n",
      "        [-0.0198],\n",
      "        [-0.0200]])\n",
      "tensor([[-0.0149],\n",
      "        [-0.0167],\n",
      "        [-0.0096],\n",
      "        ...,\n",
      "        [-0.0147],\n",
      "        [-0.0206],\n",
      "        [-0.0158]])\n",
      "tensor([[-0.0141],\n",
      "        [-0.0116],\n",
      "        [-0.0165],\n",
      "        ...,\n",
      "        [-0.0183],\n",
      "        [-0.0023],\n",
      "        [-0.0173]])\n",
      "tensor([[-0.0112],\n",
      "        [-0.0133],\n",
      "        [-0.0064],\n",
      "        ...,\n",
      "        [-0.0129],\n",
      "        [-0.0181],\n",
      "        [-0.0202]])\n",
      "tensor([[-0.0079],\n",
      "        [-0.0133],\n",
      "        [-0.0042],\n",
      "        ...,\n",
      "        [-0.0185],\n",
      "        [-0.0074],\n",
      "        [-0.0035]])\n",
      "tensor([[-0.0126],\n",
      "        [-0.0099],\n",
      "        [-0.0088],\n",
      "        ...,\n",
      "        [-0.0114],\n",
      "        [-0.0072],\n",
      "        [-0.0157]])\n",
      "tensor([[-0.0045],\n",
      "        [-0.0122],\n",
      "        [-0.0136],\n",
      "        ...,\n",
      "        [-0.0123],\n",
      "        [-0.0069],\n",
      "        [-0.0101]])\n",
      "tensor([[-0.0067],\n",
      "        [-0.0083],\n",
      "        [-0.0093],\n",
      "        ...,\n",
      "        [-0.0050],\n",
      "        [-0.0124],\n",
      "        [-0.0060]])\n",
      "tensor([[-0.0095],\n",
      "        [-0.0079],\n",
      "        [-0.0155],\n",
      "        ...,\n",
      "        [-0.0164],\n",
      "        [-0.0085],\n",
      "        [-0.0151]])\n",
      "tensor([[-0.0110],\n",
      "        [-0.0088],\n",
      "        [-0.0099],\n",
      "        ...,\n",
      "        [-0.0093],\n",
      "        [-0.0095],\n",
      "        [-0.0090]])\n",
      "tensor([[-0.0083],\n",
      "        [-0.0123],\n",
      "        [-0.0068],\n",
      "        ...,\n",
      "        [-0.0048],\n",
      "        [-0.0109],\n",
      "        [-0.0141]])\n",
      "tensor([[-0.0025],\n",
      "        [-0.0078],\n",
      "        [-0.0025],\n",
      "        ...,\n",
      "        [-0.0042],\n",
      "        [-0.0028],\n",
      "        [-0.0096]])\n",
      "tensor([[ 0.0218],\n",
      "        [-0.0097],\n",
      "        [-0.0100],\n",
      "        ...,\n",
      "        [-0.0063],\n",
      "        [-0.0034],\n",
      "        [-0.0056]])\n",
      "tensor([[-0.0119],\n",
      "        [-0.0108],\n",
      "        [-0.0066],\n",
      "        ...,\n",
      "        [-0.0070],\n",
      "        [-0.0051],\n",
      "        [-0.0047]])\n",
      "tensor([[-0.0069],\n",
      "        [-0.0082],\n",
      "        [-0.0115],\n",
      "        ...,\n",
      "        [-0.0032],\n",
      "        [-0.0063],\n",
      "        [-0.0037]])\n",
      "################################\n",
      "Episode: 20, score: 0.140000\n",
      "0.14000001456588507\n",
      "tensor([[-0.0098],\n",
      "        [-0.0131],\n",
      "        [-0.0031],\n",
      "        ...,\n",
      "        [-0.0173],\n",
      "        [-0.0116],\n",
      "        [-0.0105]])\n",
      "tensor([[-0.0023],\n",
      "        [-0.0144],\n",
      "        [-0.0101],\n",
      "        ...,\n",
      "        [-0.0026],\n",
      "        [-0.0032],\n",
      "        [-0.0067]])\n",
      "tensor([[-0.0010],\n",
      "        [-0.0125],\n",
      "        [-0.0146],\n",
      "        ...,\n",
      "        [-0.0072],\n",
      "        [-0.0063],\n",
      "        [-0.0066]])\n",
      "tensor([[-0.0043],\n",
      "        [-0.0159],\n",
      "        [-0.0089],\n",
      "        ...,\n",
      "        [-0.0047],\n",
      "        [-0.0008],\n",
      "        [-0.0054]])\n",
      "tensor([[-0.0131],\n",
      "        [-0.0106],\n",
      "        [-0.0065],\n",
      "        ...,\n",
      "        [-0.0030],\n",
      "        [-0.0061],\n",
      "        [-0.0007]])\n",
      "tensor([[-0.0102],\n",
      "        [-0.0121],\n",
      "        [-0.0151],\n",
      "        ...,\n",
      "        [-0.0052],\n",
      "        [-0.0052],\n",
      "        [-0.0026]])\n",
      "tensor([[-0.0100],\n",
      "        [-0.0098],\n",
      "        [-0.0069],\n",
      "        ...,\n",
      "        [-0.0065],\n",
      "        [-0.0031],\n",
      "        [-0.0073]])\n",
      "tensor([[ 0.1206],\n",
      "        [-0.0053],\n",
      "        [-0.0020],\n",
      "        ...,\n",
      "        [-0.0087],\n",
      "        [-0.0032],\n",
      "        [-0.0025]])\n",
      "tensor([[ 0.0819],\n",
      "        [-0.0038],\n",
      "        [-0.0113],\n",
      "        ...,\n",
      "        [-0.0059],\n",
      "        [-0.0070],\n",
      "        [-0.0106]])\n",
      "tensor([[-0.0067],\n",
      "        [-0.0067],\n",
      "        [-0.0061],\n",
      "        ...,\n",
      "        [-0.0018],\n",
      "        [-0.0005],\n",
      "        [-0.0037]])\n",
      "tensor([[-0.0064],\n",
      "        [-0.0117],\n",
      "        [-0.0052],\n",
      "        ...,\n",
      "        [-0.0087],\n",
      "        [-0.0021],\n",
      "        [-0.0082]])\n",
      "tensor([[-0.0018],\n",
      "        [-0.0057],\n",
      "        [-0.0051],\n",
      "        ...,\n",
      "        [-0.0051],\n",
      "        [-0.0048],\n",
      "        [-0.0019]])\n",
      "tensor([[ 0.0345],\n",
      "        [-0.0066],\n",
      "        [ 1.1830],\n",
      "        ...,\n",
      "        [-0.0084],\n",
      "        [-0.0079],\n",
      "        [-0.0033]])\n",
      "tensor([[-0.0082],\n",
      "        [-0.0099],\n",
      "        [-0.0153],\n",
      "        ...,\n",
      "        [-0.0048],\n",
      "        [-0.0055],\n",
      "        [-0.0072]])\n",
      "tensor([[-0.0076],\n",
      "        [-0.0089],\n",
      "        [-0.0119],\n",
      "        ...,\n",
      "        [-0.0149],\n",
      "        [-0.0009],\n",
      "        [-0.0092]])\n",
      "tensor([[-0.0049],\n",
      "        [-0.0137],\n",
      "        [-0.0175],\n",
      "        ...,\n",
      "        [-0.0036],\n",
      "        [-0.0087],\n",
      "        [-0.0033]])\n",
      "tensor([[-0.0047],\n",
      "        [-0.0103],\n",
      "        [-0.0071],\n",
      "        ...,\n",
      "        [-0.0022],\n",
      "        [-0.0075],\n",
      "        [-0.0074]])\n",
      "tensor([[-0.0051],\n",
      "        [-0.0063],\n",
      "        [-0.0014],\n",
      "        ...,\n",
      "        [-0.0064],\n",
      "        [-0.0029],\n",
      "        [-0.0094]])\n",
      "tensor([[-0.0039],\n",
      "        [-0.0080],\n",
      "        [-0.0120],\n",
      "        ...,\n",
      "        [-0.0050],\n",
      "        [-0.0031],\n",
      "        [-0.0064]])\n",
      "tensor([[-0.0114],\n",
      "        [-0.0125],\n",
      "        [-0.0112],\n",
      "        ...,\n",
      "        [-0.0117],\n",
      "        [-0.0253],\n",
      "        [-0.0030]])\n",
      "################################\n",
      "Episode: 40, score: 0.130000\n",
      "0.1300000138580799\n",
      "tensor([[-0.0127],\n",
      "        [-0.0101],\n",
      "        [-0.0091],\n",
      "        ...,\n",
      "        [-0.0073],\n",
      "        [-0.0064],\n",
      "        [-0.0109]])\n",
      "tensor([[-0.0064],\n",
      "        [-0.0055],\n",
      "        [ 0.0400],\n",
      "        ...,\n",
      "        [-0.0061],\n",
      "        [-0.0041],\n",
      "        [-0.0039]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-09cc47b56d05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# First Version Separate Clipped with no parallel simple learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mnb_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclipped_surrogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDelta_t_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0moptimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mL1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-c7407ca686ae>\u001b[0m in \u001b[0;36mclipped_surrogate\u001b[1;34m(Delta_t, critic, device, policy, old_probs, actions, states, rewards, batch_size, nb_agent, discount, epsilon, beta)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# ATTENTION ENTRAINE DEUX FOIS.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# REWARD TRAINING ON BOTH REWARD.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTD_Training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Critic network training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss/Critic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miteration_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-31d130d30a3d>\u001b[0m in \u001b[0;36mTD_Training\u001b[1;34m(Critic, states, reward, actions, discount, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Simple MSE Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0moptimizer_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################################### MAIN_CODE #################################################\n",
    "# training loop max iterations\n",
    "episode = 10000\n",
    "\n",
    "\n",
    "tmax = 1000\n",
    "discount_rate = .9997\n",
    "epsilon = 0.04 # try less than 0.1?\n",
    "beta = .01\n",
    "SGD_epoch = 6\n",
    "batch_size = 128 #64\n",
    "lambd = 0.80\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "aleatoire = False\n",
    "writer.add_text(\"CONFIG\",\"aleatoire :\" + str(aleatoire) + \"tmax :\" + str(tmax) + \"batch_size :\" + str(batch_size) + \"discount_rate :\" + str(discount_rate) + \"epsilon\" + str(epsilon)+ \"beta\" + str(beta) + \"SGD_epoch :\" + str(SGD_epoch) + \"lambd :\" + str(lambd) + \"lr : 2e-4 x2\")\n",
    "iteration_all = 0\n",
    "\n",
    "# NEW VARIABLE ADDED\n",
    "itbis = 0\n",
    "scores_deque = deque(maxlen=100)\n",
    "All_av_Score = []\n",
    "for e in range(episode):\n",
    "    \n",
    "    # EVALUATION STEP\n",
    "    # collect trajectories\n",
    "    states, actions, rewards,prob,reward_episode = collect_trajectories(env, env_info, policy1, policy2, device, tmax)\n",
    "    total_rewards = np.mean(np.sum(rewards,axis=0))\n",
    "    # REWARD COMPUTATION\n",
    "    for r in reward_episode:\n",
    "        itbis+=1\n",
    "        writer.add_scalar('Score_agent1',r[0],itbis)\n",
    "        writer.add_scalar('Score_agent2',r[1],itbis)\n",
    "        scores_deque.append(np.max(r))\n",
    "        writer.add_scalar('Score_espisode_mean',np.mean(scores_deque),itbis)\n",
    "        \n",
    "    \n",
    "    # Compute advantages estimate\n",
    "    Delta_t = TD_evaluation(critic,states,actions,rewards,discount_rate,device)\n",
    "    writer.add_scalar('DeltaT',torch.mean(Delta_t),iteration_all)\n",
    "    Delta_t = GAE_evaluation(Delta_t,discount_rate,lambd)\n",
    "    writer.add_scalar('Advantage',torch.mean(Delta_t),iteration_all)\n",
    "    \n",
    "    states = torch.stack(states)[:-1]\n",
    "    actions = torch.stack(actions)[:-1]\n",
    "    prob = torch.stack(prob)[:-1]\n",
    "    rewards = np.asarray(rewards)[:-1]\n",
    "    \n",
    "    # TRAINING STEP\n",
    "    indices = torch.split(torch.from_numpy(np.arange(0,states.shape[0],1)),batch_size,0) # Make chunk of the trajectory\n",
    "    for epoch in range(SGD_epoch):\n",
    "        # TRAINING OVER THE BATCH SIZE\n",
    "        for chunks in indices:\n",
    "            iteration_all += 1\n",
    "            chunk = chunks.long()\n",
    "            chunk_numpy = chunk.numpy().astype('int')\n",
    "\n",
    "            states_chunk = states[chunk]\n",
    "            actions_chunk = actions[chunk]\n",
    "            prob_chunk = prob[chunk]\n",
    "            rewards_chunk = rewards[chunk_numpy]\n",
    "            Delta_t_chunk = Delta_t[chunk]\n",
    "            rewards_chunk = rewards_chunk.tolist()\n",
    "            \n",
    "            # First Version Separate Clipped with no parallel simple learning\n",
    "            nb_agent = 0\n",
    "            L1 = clipped_surrogate(Delta_t_chunk,critic,device,policy1,prob_chunk,actions_chunk, states_chunk, rewards_chunk,batch_size,nb_agent, epsilon=epsilon, beta=beta)\n",
    "            optimizer1.zero_grad()\n",
    "            L1.backward()\n",
    "            optimizer1.step()\n",
    "            \n",
    "            nb_agent = 1\n",
    "            L2 = clipped_surrogate(Delta_t_chunk,critic,device,policy2,prob_chunk,actions_chunk, states_chunk, rewards_chunk,batch_size, nb_agent,epsilon=epsilon, beta=beta)\n",
    "            optimizer2.zero_grad()\n",
    "            L2.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            writer.add_scalar('Loss/Policy1',L1,iteration_all)\n",
    "            writer.add_scalar('Loss/Policy2',L2,iteration_all)\n",
    "            del L1\n",
    "            del L2\n",
    "    writer.add_scalar('Score',total_rewards,e)\n",
    "            \n",
    "    mean_rewards.append(total_rewards)\n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"################################\")\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,total_rewards))\n",
    "        print(total_rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy1.state_dict(), 'PPO_actor1_stable.pth')\n",
    "torch.save(policy2.state_dict(), 'PPO_actor2_stable.pth')\n",
    "torch.save(critic.state_dict(), 'PPO_critic_stable.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Navigation3",
   "language": "python",
   "name": "navigation3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
